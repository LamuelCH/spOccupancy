---
title: "Fitting occupancy models with `spOccupancy`"
author: "Jeffrey W. Doser"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 3
bibliography: [references.bib]
biblio-style: apalike
vignette: >
  %\VignetteIndexEntry{modelFitting}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
options(rmarkdown.html_vignette.check_title = FALSE)
knitr::opts_chunk$set(
  comment = "", eval = FALSE
)
```

## Introduction

This vignette provides worked examples and explanations on fitting single species and multispecies occupancy models available in the `spOccupancy` R package. We will provide step by step examples on how to fit the following models: 

1. Occupancy model using `PGOcc`. 
2. Spatial occupancy model using `spPGOcc`. 
3. Multispecies occupancy model using `msPGOcc`.
4. Spatial multispecies occupancy model using `spMsPGOcc`.
5. Integrated occupancy model using `intPGOcc`.
6. Spatial integrated occupancy model using `spIntPGOcc`. 

Statistical details of each model can be found in the Gibbs sampler vignette. We will also show how `spOccupancy` provides functions for posterior predictive checks as a Goodness of Fit assessment, model comparison and assessment using the Widely Applicable Information Criterion (WAIC), and out of sample predictions using standard R helper functions (i.e., `predict`). 

To get started, we load the `spOccupancy` package, as well as the `coda` package, which we will use for some MCMC diagnostics. We will also use the `sp` and `raster` packages to create some very basic plots of our results. 

```{r setup}
library(spOccupancy)
library(coda)
library(sp)
library(raster)
```

## Example data set: Foliage-gleaning birds at Hubbard Brook

As an example data set, we will use data from twelve foliage-gleaning birds collected from point count surveys at Hubbard Brook Experimental Forest (HBEF) in New Hampshire, USA. Specific details on the data set are available on the [Hubbard Brook website](https://portal.edirepository.org/nis/mapbrowse?scope=knb-lter-hbr&identifier=178) and @doser2021ICOM. The data are provided in the `spOccupancy` package and are loaded with `data(hbef2015)`. 

```{r}
data(hbef2015)
str(hbef2015)
```

The object `hbef2015` is a list comprised of the detection-nondetection data (`y`), covariates on the occupancy portion of the model (`occ.covs`), covariates on the detection portion of the model (`det.covs`), and the spatial coordinates of each site for use in spatially-explicit models. This list is in the exact format required for input to `spOccupancy` model functions. `hbef2015` contains data on 12 species in the three-dimensional array `y`. Here we will use data on the Ovenbird (OVEN) to display single species models, so let's go ahead and subset the `hbef2015` list to only include data from OVEN in a new object `ovenHBEF`. 

```{r}
sp.names <- attr(hbef2015$y, "dimnames")[[1]]
ovenHBEF <- hbef2015
ovenHBEF$y <- ovenHBEF$y[sp.names == "OVEN", , ]
table(ovenHBEF$y)
```

We see OVEN is detected at around half of all site-replicate combinations.

## `PGOcc`: Single species occupancy models

The `PGOcc` function fits single species occupancy models using Polya-Gamma latent variables, which should make it faster than other implementations of occupancy models using a logit link function [@clark2019, @polson2013]. `PGOcc` has the following arguments: 

```{r, eval = FALSE}
PGOcc(occ.formula, det.formula, data, starting, n.samples, priors, 
      n.omp.threads = 1, verbose = TRUE, n.report = 100, 
      n.burn = round(.10 * n.samples), n.thin = 1, ...)
```

The first two arguments, `occ.formula` and `det.formula`, use standard R model syntax to denote the covariates included in the occupancy and detection portions of the model, respectively. Only the right hand side of the formulas are included. The names of variables given in the formulas should correspond to those found in `data`, which is a list consisting of the following tags: `y` (detection-nondetection data), `occ.covs` (occupancy covariates), `det.covs` (detection covariates). `y` should be stored as a sites x replicate matrix, `occ.covs` as a matrix or data frame with site-specific covariate values, and `det.covs` is a list with each list element corresponding to a covariate to include in the detection portion of the model. Covariates on detection can vary by site and/or survey. The `ovenHBEF` data are already in the required format. Here we envision OVEN occurrence as a function of linear and quadratic elevation and will include three observational covariates (linear and quadratic day of survey, time of day of survey) on the detection portion of the model. We specify the formulas below 

```{r}
oven.occ.formula <- ~ Elevation + Elevation.2
oven.det.formula <- ~ day + tod + day.2
# Make sure format of ovenHBEF is correct.
str(ovenHBEF)
```

Next, we specify the starting values in `starting`. `PGOcc` will set starting values by default (and report what these values are), but here we will do this explicitly. Starting values are specified in a list with the following tags: `z` (latent occupancy values), `alpha` (detection regression coefficients), `beta` (occupancy regression coefficients). Below we specify set all initial values of the regression coefficients to 0, and set starting values for `z` based on the detection-nondetection data matrix.

```{r}
# Number of detection and occupancy parameters 
# + 1 is needed to count the intercept
p.det <- length(ovenHBEF$det.covs) + 1
p.occ <- ncol(ovenHBEF$occ.covs) + 1
oven.starting <- list(alpha = rep(0, p.det), 
                      beta = rep(0, p.occ), 
                      z = apply(ovenHBEF$y, 1, max, na.rm = TRUE))
```

We will next specify the priors for the occupancy and detection regression coefficients. The Polya-Gamma data augmentation algorithm employed by `spOccupancy` assumes normal priors for both the detection and occupancy regression coefficients. These priors are specified in a list with tags `beta.normal` for occupancy and `alpha.normal` for detection parameters. Each list element is then itself a list, with the first element of the list consisting of the hypermeans for each coefficient to be estimated and the second element of the list consisting of the hypervariances for each coefficient. By default, `spOccupancy` will set the hypermeans to 0 and the hypervariances to 2.72, which corresponds to a relatively flat prior on the probability scale (0, 1) [@broms2016model]. We will use these default priors here, but we specify them explicitly below for clarity 

```{r, tidy = FALSE}
oven.priors <- list(alpha.normal = list(mean = rep(0, p.det),
                                        var = rep(2.72, p.det)), 
                    beta.normal = list(mean = rep(0, p.occ), 
                                       var = rep(2.72, p.occ)))
```

Our last step is to specify the number of samples to run the MCMC (`n.samples`), the amount of burn-in (`n.burn`), and how often we want to thin the posterior samples (`n.thin`). For a simple single species occupancy model, we should only need a small amount of burn-in and very minimal thinning (if any). 

```{r}
n.samples <- 3000
n.burn <- 1000
n.thin <- 1
```

We are now set to run the occupancy model. Single species occupancy models are fast, and so we set `n.omp.threads = 1` to indicate we won't use multiple threads to run the model. We set `verbose = TRUE` and `n.report = 1000` to report progress after every 1000th MCMC iteration. 

```{r}
n.samples <- 3000
out <- PGOcc(occ.formula = oven.occ.formula, 
             det.formula = oven.det.formula, 
             data = ovenHBEF, 
             starting = oven.starting, 
             n.samples = n.samples, 
             priors = oven.priors, 
             n.omp.threads = 1, 
             verbose = TRUE, 
             n.report = 1000, 
             n.burn = n.burn, 
             n.thin = n.thin)
str(out)
```

You may see a slightly different message depending on whether or not your computer compiled `spOccupancy` with OpenMP support. We see `PGOcc` returns a list of class `PGOcc` with a suite of different objects, most of them being `coda::mcmc` objects of posterior samples. Notice the "Preparing the data" printed section doesn't have any information shown in it. `spOccupancy` functions will present warnings when preparing the data for the model in this section, or will print out the default priors or starting values used when they are not specified in the function call. Here we specified everything explicitly so no information was reported. 

For a nice summary of the regression parameters we can use the `summary` function on the resulting `PGOcc` object. 

```{r}
summary(out)
```

We see OVEN is fairly prominent in the forest given the large intercept value, and the negative linear and quadratic terms for `Elevation` suggest occurrence probability peaks at mid-elevations. 

### Convergence diagnostics

The posterior samples in the `PGOcc` object are `coda::mcmc` objects, which we can quickly assess for convergence visually using trace plots. 

```{r, fig.height = 4, fig.width = 4, fig.align = 'center'}
plot(out$beta.samples, density = FALSE)
```
```{r, fig.height = 4, fig.width = 4, fig.align = 'center'}
plot(out$alpha.samples, density = FALSE)
```

For a complete analysis (i.e., in a peer-reviewed manuscript), we will likely want to more formally check for convergence, perhaps using the Gelman-Rubin R-hat diagnostic. This requires running multiple chains at largely different starting values for the regression parameters. For a single species non-spatial occupancy model, we can accomplish this by running multiple chains sequentially (since they run really fast) with different starting values, then combining the output into a `coda::mcmc.list` object for use the `coda::gelman.diag` function. Notice below we set `verbose = FALSE` to suppress the messages printed by `PGOcc`. 

```{r}
oven.starting <- list(alpha = rep(2, p.det), 
                      beta = rep(2, p.occ), 
                      z = apply(ovenHBEF$y, 1, max, na.rm = TRUE))
out.2 <- PGOcc(occ.formula = oven.occ.formula, 
               det.formula = oven.det.formula, 
               data = ovenHBEF, 
               starting = oven.starting, 
               n.samples = n.samples, 
               priors = oven.priors, 
               n.omp.threads = 1, 
               verbose = FALSE, 
               n.report = 1000, 
	       n.burn = n.burn, 
	       n.thin = n.thin)
oven.starting <- list(alpha = rep(-2, p.det), 
                      beta = rep(-2, p.occ), 
                      z = apply(ovenHBEF$y, 1, max, na.rm = TRUE))
out.3 <- PGOcc(occ.formula = oven.occ.formula, 
               det.formula = oven.det.formula, 
               data = ovenHBEF, 
               starting = oven.starting, 
               n.samples = n.samples, 
               priors = oven.priors, 
               n.omp.threads = 1, 
               verbose = FALSE, 
               n.report = 1000, 
               n.burn = n.burn, 
               n.thin = n.thin)
# beta convergence
gelman.diag(mcmc.list(out$beta.samples, out.2$beta.samples, 
		      out.3$beta.samples))
# alpha convergence
gelman.diag(mcmc.list(out$alpha.samples, out.2$alpha.samples, 
		      out.3$alpha.samples))
```

### Posterior predictive checks

The function `ppcOcc` performs a posterior predictive check on all `spOccupancy` model objects as a Goodness of Fit (GoF) assessment. The fundamental idea of a posterior predictive check is as follows: our model should generate data that closely align with the observed data. If there are drastic differences in the true data from the model generated data, our model likely is not very useful [@hobbs2015]. GoF assessments are more complicated using binary data, like detection-nondetection used in occupancy models, as standard approaches are not valid assessments for binary data [@broms2016model, @mccullagh2019]. Thus, any approach to assess model fit for detection-nondetection data must bin the raw values in some manner, and then perform a model fit assessment on the binned values. There are numerous ways we could envision binning the raw detection-nondetection values [@kery2015applied].

In `spOccupancy`, model fitted values are calculated directly by the model fitting function (i.e., in this case, `PGOcc`) and can be extracted from the resulting model object using the `fitted` function for subsequent analysis. The resulting `PGOcc` model object is sent as input to the `ppcOcc` function, along with a fit statistic (`fit.stat`) and numeric value indicating how to group the data (`group`). Currently supported fit statistics include the Freeman-Tukey statistic and the Chi-Square statistic (`freeman-tukey` or `chi-square`, respectively, @kery2015applied). Currently, `ppcOcc` allows the user to group the data by row (site; `group = 1`) or column (replicate; `group = 2`). `ppcOcc` will then return a set of posterior samples for the fit statistic (or discrepancy measure) using the observed data (`fit.y`) and model generated data set (`fit.y.rep`), summed across all data points. These values can be used with the `summary` function to generate a Bayesian p-value. Bayesian p-values are sensitive to individual values, so we should also explore the discrepancy measures for each "grouped" data point. `ppcOcc` returns a matrix of posterior quantiles for the fit statistic for both the observed (`fit.y.group.quants`) and model generated data (`fit.y.rep.group.quants`) for each "grouped" data point. 

We next perform a posterior predictive check using the Freeman-Tukey statistic grouping the data by sites. We summarize the posterior predictive check with the `summary` function, which reports a Bayesian p-value. A Bayesian p-value that hovers around 0.5 indicates adequate model fit, while values less than 0.1 or greater than 0.9 suggest our model does not fit the data well [@hobbs2015]. 

```{r}
ppc.out <- ppcOcc(out, fit.stat = 'freeman-tukey', group = 1)
summary(ppc.out)
```

The Bayesian p-value is the proportion of posterior samples of the fit statistic of the model generated data that are greater than the corresponding fit statistic of the true data, summed across all "grouped" data points. We can create a visual representation of the Bayesian p-value as follows, which is highly motivated by @kery2015applied. 

```{r, fig.width = 4, fig.height = 4, fig.align = 'center'}
ppc.df <- data.frame(fit = ppc.out$fit.y, 
		     fit.rep = ppc.out$fit.y.rep, 
		     color = 'lightskyblue1')
ppc.df$color[ppc.df$fit.rep > ppc.df$fit] <- 'lightsalmon'
plot(ppc.df$fit, ppc.df$fit.rep, bg = ppc.df$color, pch = 21, 
     ylab = 'Fit', xlab = 'True')
lines(ppc.df$fit, ppc.df$fit, col = 'black')
```

However, relying solely on the Bayesian p-value as an assessment of model fit is not always a great option, as individual data points can have an overbearing influence on the resulting summary value. Instead of summing across all data points for a single discrepancy measure, `ppcOcc` also allows us to explore discrepancy measures on a "grouped" point by point basis, which allows us to better figure out where our model fits well and where it does not. The resulting `ppcOcc` object will contain the objects `fit.y.group.quants` and `fit.y.rep.group.quants`, which contain quantiles of the posterior distributions for the discrepancy measures of each grouped data point. Below we plot the difference in the discrepancy measure between the fitted and true data across each of the sites. 

```{r, fig.width = 4, fig.height = 4, fig.align = 'center'}
diff.fit <- ppc.out$fit.y.rep.group.quants[3, ] - ppc.out$fit.y.group.quants[3, ]
plot(diff.fit, pch = 19, xlab = 'Site ID', ylab = 'Fitted - True Discrepancy')
```

We see there are a few sites where the true discrepancy is much larger than the discrepancy under the fitted data. Here we will ignore this, but in a real analysis we would explore these sites further to see what could explain this pattern (e.g., are the sites close together in space?). 

### Model selection using WAIC

Posterior predictive checks allow us to assess how well our model fits the data, but they are not very useful if we want to compare multiple competing models and ultimately select a final model based on some criterion. Bayesian model selection is very much a constantly changing field, especially in the ecological and environmental sciences. See @hooten2015guide for an accessible overview of Bayesian model selection for ecologists. 

For Bayesian hierarchical models like occupancy models, the most common Bayesian model selection criterion, DIC, is not applicable [@hooten2015guide]. Instead, we can use the Widely Applicable Information Criterion [@watanabe2010] to compare a set of models and select the best performing model according to the WAIC for final analysis. When focused primarily on predictive performance, an out-of-sample validation approach is another attractive (but more computationally intensive) alternative to compare a series of models. We will show how to perform out of sample model validation using `spOccupancy` model objects in a subsequent vignette. 

The WAIC is calculated for all `spOccupancy` model objects using the function `waicOcc`. We calculate the WAIC as 

$$
\text{WAIC} = -2 \times (\text{elpd} - \text{pD}), 
$$

where elpd is the expected log pointwise predictive density and PD is the effective number of parameters. We calculate elpd by calculating the likelihood for each posterior sample, taking the mean of these likelihoods, taking the log of the mean of the likelihoods, and summing these values across all sites. We calculate the effective number of parameters by calculating the variance of the log likelihood for each site taken over all posterior samples, and then summing these values across all sites.

We calculate the WAIC using `waicOcc` for our OVEN model below.

```{r}
waicOcc(out)
```

Next we rerun the OVEN model, but this time we assume occurrence is constant across the HBEF, and subsequently compare the WAIC value to the full model

```{r}
p.occ <- 1
oven.starting <- list(alpha = rep(0, p.det), 
                      beta = rep(0, p.occ), 
                      z = apply(ovenHBEF$y, 1, max, na.rm = TRUE))
oven.priors <- list(alpha.normal = list(mean = rep(0, p.det), 
                                        var = rep(2.72, p.det)), 
                    beta.normal = list(mean = rep(0, p.occ), 
                                       var = rep(2.72, p.occ)))
out.small <- PGOcc(occ.formula = ~ 1, 
		   det.formula = oven.det.formula, 
		   data = ovenHBEF, 
		   starting = oven.starting, 
		   n.samples = n.samples, 
		   priors = oven.priors, 
		   n.omp.threads = 1, 
		   verbose = FALSE, 
		   n.burn = n.burn, 
		   n.thin = n.thin)
waicOcc(out.small)
```

Smaller values of WAIC indicate models with better performance. We see the WAIC for the model with elevation is smaller than the intercept only model, indicating elevation is an important predictor for OVEN occurrence in HBEF. 

### Prediction

All resulting model objects from `spOccupancy` model functions can be used with the `predict` function to generate a series of posterior predictive samples at non-sampled locations, given the values of all covariates used in the model fitting process. The object `hbefElev` contains elevation values at a 30x30m resolution from the National Elevation Dataset across the entire HBEF. The values are standardized using the mean and standard deviation of the elevation values used to fit the model. We load the data below

```{r}
data(hbefElev)
str(hbefElev)
```

The column `val` contains the standardized elevation values, while `Easting` and `Northing` contain the spatial coordinates that we will use for plotting. We can obtain posterior predictive samples for the occurrence probabilities at these sites by using the `predict` function and our `PGOcc` model object. 

```{r}
X.0 <- cbind(1, hbefElev$val, hbefElev$val^2)
out.pred <- predict(out, X.0)
```

For `PGOcc` objects, the `predict` function takes two arguments: (1) the `PGOcc` model object; and (2) a matrix or data frame consisting of the design matrix for the prediction locations (including an intercept). The resulting object consists of posterior predictive samples for the latent occurrence probabilities (`psi.0.samples`) and latent occurrence values (`z.0.samples`). The beauty of the Bayesian paradigm is that these predictions all have fully propagated uncertainty. We can use these values to create plots of the predicted mean occurrence values, as well as their standard deviation.

```{r, fig.width = 5, fig.height = 5, fig.align = 'center'}
plot.dat <- data.frame(x = hbefElev$Easting, 
		       y = hbefElev$Northing, 
		       mean.psi = apply(out.pred$psi.0.samples, 2, mean), 
		       sd.psi = apply(out.pred$psi.0.samples, 2, sd))

test <- rasterFromXYZ(plot.dat)
plot(test[[1]], main = 'Mean OVEN occurrence probability', xlab = 'Easting', 
     ylab = 'Northing')
plot(test[[2]], main = 'SD OVEN occurrence probability', xlab = 'Easting', 
     ylab = 'Northing')
```

## `spPGOcc`: single species spatial occupancy models

When working across large spatial domains, accounting for residual spatial autocorrelation in species distributions can often improve predictive performance, leading to more accurate species distribution maps [@guelat2018, lany2020]. The function `spPGOcc` fits single species spatial occupancy models using Polya-Gamma latent variables, where spatial autocorrelation is accounted for using a spatial Gaussian Process.

Perhaps one of the reasons why spatially-explicit occupancy models are not widely used is due to the large run times required when running occupancy models across large regions. This problem is known as the "big N" problem in the spatial statistics literature [@heaton2019case]. When the number of sites is greater than say 1000, spatial Gaussian process models can be drastically slow as a result of needing to take the inverse of the spatial covariance matrix at each MCMC iteration. Numerous approximation methods exist to reduce this computational cost [@heaton2019case]. `spPGOcc` fits spatial occupancy models using either a full Gaussian process or a Nearest Neighbor Gaussian Process (NNGP) [@datta2016hierarchical, @finley2019efficient], which provides nearly identical results to the full Gaussian process at a fraction of the computational cost. See @datta2016hierarchical for statistical details on the NNGP, as well as @finley2020spnngp for details on using NNGPs with Polya-Gamma latent variables. 

We will fit the same occupancy model for OVEN that we fit previously using `PGOcc`, but we will now make the model spatially explicit by incorporating a spatial process with `spPGOcc`. First, let's take a look at the arguments for `spPGOcc`: 

```{r, eval = FALSE}
spPGOcc(occ.formula, det.formula, data, starting, n.batch, 
        batch.length, accept.rate = 0.43, priors,
        cov.model = "exponential", tuning, n.omp.threads = 1, 
        verbose = TRUE, NNGP = FALSE, n.neighbors = 15, 
        search.type = "cb", n.report = 100, 
        n.burn = round(.10 * n.batch * batch.length), 
        n.thin = 1, ...)
```

We will walk through each of the arguments to `spPGOcc` in the context of our Ovenbird example. The occupancy (`occ.formula`) and detection (`det.formula`) formulas, as well as the list of data (`data`), take the same form as with `PGOcc`. Notice the `coords` matrix in the `ovenHBEF` list of data. We did not use this for `PGOcc` but specifying the spatial coordinates in `data` is required for all spatially explicit models in `spOccupancy`. 

```{r}
oven.occ.formula <- ~ Elevation + Elevation.2
oven.det.formula <- ~ day + tod + day.2
str(ovenHBEF) # coords is required for spPGOcc.
```

The starting values (`starting`) are again specified in a list. Valid tags for starting values now additionally include the parameters associated with the spatial random effects. These include: `sigma.sq` (spatial variance parameter), `phi` (spatial range parameter), `w` (the latent spatial random effects at each site), and `nu` (spatial smoothness parameter). `nu` is only specified if using a Matern covariance function (i.e., `cov.model = 'matern'`). `spOccupancy` supports four spatial covariance models (`exponential`, `spherical`, `gaussian`, and `matern`), which are specified in the `cov.model` argument. Here we will use an exponential covariance model. As a starting value for the spatial range parameter `phi`, we compute the mean distance between points in HBEF and then set it equal to 3 divided by this mean distance. When using an exponential covariance function, $\frac{3}{\phi}$ is the effective range, or the distance at which the residual spatial correlation between two sites is 0.05. Thus our initial guess for this effective range is the average distance betweeen sites across HBEF. 

```{r, tidy = FALSE}
# Distances between sites
dist.hbef <- dist(ovenHBEF$coords)
# Number of detection and occupancy regression parameters
p.det <- length(ovenHBEF$det.covs) + 1
p.occ <- ncol(ovenHBEF$occ.covs) + 1
# Exponential covariance model
cov.model <- "exponential"
oven.starting <- list(alpha = rep(0, p.det), 
                      beta = rep(0, p.occ), 
                      z = apply(ovenHBEF$y, 1, max, na.rm = TRUE), 
                      sigma.sq = 2, 
                      phi = 3 / mean(dist.hbef), 
                      w = rep(0, nrow(ovenHBEF$y)))
```

The next three arguments (`n.batch`, `batch.length`, and `accept.rate`) are all related to the Adaptive MCMC sampler we use to fit the model. Updates for the spatial range parameter (and smoothness parameter if `cov.model = matern`) require the use of a Metropolis Hastings algorithm. We implement an adaptive Metropois-Hastings algorithm discussed in @roberts2009examples. This algorithm adjusts the tuning values for each parameter that requires a Metropolis-Hastings update within the sampler itself. This process results in a more efficient sampler than if we were to fix the tuning parameters prior to fitting the model. The parameter `accept.rate` is the target acceptance rate for each parameter, and the algorithm will adjust the tuning parameters to hover around this value. The default value is 0.43, which we suggest leaving as is unless you have a good reason to change it. The tuning parameters are updated after a single "batch". We must specify the total `n.batch` batches, where each "batch" consists of `batch.length` MCMC samples. Thus, the total number of MCMC samples is `n.batch * batch.length`. Typically, we set `batch.length = 25` and then play around with `n.batch` until convergence is reached. Here we set `n.batch = 400` for a total of 10000 MCMC samples. We will additionally specify a burn-in period of 2000 samples and a thinning rate of 4. We also need to specify an initial value for the tuning parameters for the spatial decay and smoothness parameters (if applicable). These values are sent as input in the form of a list with tags `phi` and `nu`. The initial tuning value can be any value greater than 0, but we recommend starting the value out around 0.5. After some initial runs of the model, if you notice the final acceptance rate of a parameter is much larger than the target acceptance rate (`accept.rate`), you can then change the initial tuning value to get closer to the target rate. 

```{r}
batch.length <- 25
n.batch <- 400
n.burn <- 2000
n.thin <- 4
oven.tuning <- list(phi = 0.5)
```

Priors are again specified in a list in the argument `priors`. We assume an inverse gamma prior for the spatial variance parameter `sigma.sq` (tag is `sigma.sq.ig`), and uniform priors for the spatial decay parameter `phi` and smoothness parameter `nu` (if Matern), with the associated tags `phi.unif` and `nu.unif`. The hyperparameters of the inverse Gamma are passed as a vector of length two, with the first and second elements corresponding to the shape and scale, respectively. The lower and upper bounds of the uniform distribution are passed in as a two-element vector for the uniform priors. 

The priors for the spatial parameters in a spatially-explicit model must be at least weakly informative for the model to converge [@banerjee2003]. For the inverse-Gammma prior on the spatial variance, we typically set the shape parameter to 2 and the scale parameter equal to our best guess of the spatial variance. For the spatial decay parameter, we determine the bounds of the uniform distribution by computing the smallest distance between sites and the largest distance between sites. We then set the lower bound of the uniform to `3/max` and the upper bound to `3/min`, where min and max correspond to the predetermined distances between sites. 

```{r}
# Minimum value is 0, so need to grab second smallest
min.dist <- sort(unique(dist.hbef))[2]
max.dist <- max(dist.hbef)
oven.priors <- list(beta.normal = list(mean = rep(0, p.occ), 
                                       var = rep(2.72, p.occ)), 
                    alpha.normal = list(mean = rep(0, p.det), 
                                        var = rep(2.72, p.det)), 
                    sigma.sq.ig = c(2, 2), 
                    phi.unif = c(3/max.dist, 3/min.dist))
```

The argument `n.omp.threads` specifies the number of threads to use for parallelization, while `verbose` specifies whether or not to print the progress of the sampler. We *highly* recommend setting `verbose = TRUE` for all spatial models to ensure the adaptive MCMC is working as you want. The argument `n.report` specifies the interval to report the Metropolis sampler acceptance. Note that `n.report` is specified in terms of batches, not the overall number of samples. Below we set `n.report = 100`, which will result in information on the acceptance rate and tuning parameters every 100th batch. 

```{r}
n.omp.threads <- 1
verbose <- TRUE
n.report <- 100
```

The remaining parameters (`NNGP`, `n.neighbors` and `search.type`) relate to whether or not you want to fit the model with a Gaussian Process (GP) or NNGP. The argument `NNGP` is a logical value indicating whether to fit the model with an NNGP (`TRUE`) or a regular GP (`FALSE`). For data sets that have more than 1000 locations, using an NNGP will have substantial increases in run time. Even for more modest size data sets (like the HBEF data set), using an NNGP will be quite a bit faster. Unless you are concerned about the NNGP approximation for some reason, we recommend setting `NNGP = TRUE`. The argument `n.neighbors` and `search.type` specify the number of neighbors used in the NNGP and the nearest neighbor search algorithm, respectively, to use for the NNGP model. Generally, the default values of these arguments will be adequate. @datta2016hierarchical showed that setting `n.neighbors = 15` is usually sufficient, although for certain data sets a good approximation can be achieved with as small as five neighbors, which could substantially decrease run time. We generally recommend leaving `search.type = "cb"`, as this results in a fast code book nearest neighbor search algorithm. However, details on when you may want to change this are described in @finley2020spnngp. We will run an NNGP model using the default values for `n.neighbors` and `search.type`, and so we won't explicitly specify them in the model call. 

We now fit the model and summarize the results using `summary`. 

```{r}
out.sp <- spPGOcc(occ.formula = oven.occ.formula, 
		  det.formula = oven.det.formula, 
		  data = ovenHBEF, 
		  starting = oven.starting, 
		  n.batch = n.batch, 
		  batch.length = batch.length, 
		  priors = oven.priors, 
		  cov.model = cov.model, 
		  NNGP = TRUE, 
		  tuning = oven.tuning, 
		  n.report = n.report, 
		  n.burn = n.burn, 
		  n.thin = n.thin)
str(out.sp)
summary(out.sp)
```

We see `spPGOcc` returns a list of class `spPGOcc` and consists of posterior samples for all parameters. Note that posterior samples for spatial parameters are stored in the list element `theta.samples`. The `summary` function reveals model results generally align with those found using the non-spatial model. 

### Convergence diagnostics

Convergence diagnostics, posterior predictive checks, model selection, and out-of-sample prediction all proceed analogously to what we saw with the non-spatial occupancy model using `PGOcc`. 

```{r, fig.height = 4, fig.width = 4, fig.align = 'center'}
plot(out.sp$beta.samples, density = FALSE)
plot(out.sp$alpha.samples, density = FALSE)
plot(out.sp$theta.samples, density = FALSE)
```

We might want to run the chain for a bit longer to ensure convergence of the spatial parameters, but we'll resist doing so for now. Convergence can be more formally assessed using the Gelman-Rubin diagnostic as done for the nonspatial model. 

### Posterior predictive checks

For our posterior predictive check, we send the `spPGOcc` model object to the `ppcOcc` function. 

```{r}
ppc.sp.out <- ppcOcc(out.sp, fit.stat = 'freeman-tukey', group = 1)
summary(ppc.sp.out)
```

The Bayesian p-value does not suggest any lack of fit. 

### Model selection using WAIC 

We next use the `waicOcc` function to compute the WAIC, which we can compare to the non-spatial model to assess the benefit of incorporating the spatial random effects. 

```{r}
waicOcc(out.sp)
# Compare to non-spatial model
waicOcc(out)
```

We see the WAIC value for the spatial model is marginally smaller than that of the nonspatial model, indicating that incorporation of the spatial random effects may not be necessary. This is not all that surprising, as we expect elevation to soak up most of the spatial variation in OVEN occurrence across the forest. 

### Prediction

Finally, we can perform out of sample prediction using the `predict` function just as before. Out of sample prediction for spatial models is more computationally intensive than non-spatial models, and so the `predict` function for `spPGOcc` class objects also has options for parallelization (`n.omp.threads`) and printing sampler progress (`verbose` and `n.report`). Note that for `spPGOcc`, you also need to supply the coordinates of the out of sample prediction locations in addition to the covariate values. 

```{r, fig.width = 5, fig.height = 5, fig.align = 'center', eval = FALSE}
coords.0 <- as.matrix(hbefElev[, c('Easting', 'Northing')])
out.sp.pred <- predict(out.sp, X.0, coords.0, verbose = TRUE, verbose = FALSE)
plot.dat <- data.frame(x = hbefElev$Easting, 
		       y = hbefElev$Northing, 
		       mean.psi = apply(out.sp.pred$psi.0.samples, 2, mean), 
		       sd.psi = apply(out.sp.pred$psi.0.samples, 2, sd))
test <- rasterFromXYZ(plot.dat)
plot(test[[1]], main = 'Mean OVEN occurrence probability', xlab = 'Easting', 
     ylab = 'Northing')
plot(test[[2]], main = 'SD OVEN occurrence probability', xlab = 'Easting', 
     ylab = 'Northing')
```

The mean predictions look nearly identical to the non-spatial model. The standard deviations for the spatial predictions are larger. 

## `msPGOcc`: Multispecies occupancy models 

`spOccupancy` uses nearly identical syntax for fitting multispecies models as it does for single species models and provides the same functionality for posterior predictive checks, GoF assessments using WAIC, and out of sample prediction. The `msPGOcc` function fits nonspatial multispecies occupancy models using Polya-Gamma latent variables, which results in substantial increases in run time compared to standard implementations of logit link multispecies occupancy models. `msPGOcc` has exactly the same arguments as `PGOcc`: 

```{r, eval = FALSE}
msPGOcc(occ.formula, det.formula, data, starting, n.samples, priors, 
	n.omp.threads = 1, verbose = TRUE, n.report = 100, 
	n.burn = round(.10 * n.samples), n.thin = 1, ...)
```

We will again use the Hubbard Brook data in `hbef2015` as an example data set, but we will now model occurrence for all 12 species in the community. Below we reload the `hbef2015` data set to get a fresh copy. 

```{r}
data(hbef2015)
```

We will model occurrence for all species as a function of linear and quadratic elevation, and detection as a function of linear and quadratic day of survey as well as the time of day the survey occurred. These models are specified in the `occ.formula` and `det.formula` as before, which reference variables stored in the `data` list. For multispecies models, the multispecies detection-nondetection data `y` is now a three-dimensional array with dimensions corresponding to species, sites, and replicates. This is how the data are provided in the `hbef2015` object, so we don't need to do any additional prep. 

```{r}
occ.ms.formula <- ~ Elevation + Elevation.2
det.ms.formula <- ~ day + tod + day.2
str(hbef2015)
```

Next we specify the starting values in `starting`. For multispecies occupancy models, we need to supply starting values for community-level and species-level parameters. In `msPGOcc`, we will supply starting values for the following parameters: `alpha.comm` (community level detection coefficients), `beta.comm` (community level occurrence coefficients), `alpha` (species level detection coefficients), `beta` (species level occurrence coefficients), `tau.beta` (community level occurrence variance parameters), `tau.alpha` (community level detection variance parameters, `z` (latent occurrence values for all species). These are all specified in a single list. Starting values for community level parameters are vectors of length corresponding to the number of community-level detection or occurrence parameters in the model (including the intercepts), while starting values for species level parameters are matrices with the number of rows indicating the number of species, and each column corresponding to a different regression parameter. The starting values for the latent occurrence matrix are specified as a matrix with $N$ rows corresponding to the number of species and $J$ columns corresponding to the number of sites. 

```{r}
N <- dim(hbef2015$y)[1]
p.det <- length(hbef2015$det.covs) + 1
p.occ <- ncol(hbef2015$occ.covs) + 1
ms.starting <- list(alpha.comm = rep(0, p.det), 
                    beta.comm = rep(0, p.occ), 
                    beta = matrix(0, N, p.occ), 
                    alpha = matrix(0, N, p.det),
                    tau.beta = rep(1, p.occ), 
                    tau.alpha = rep(1, p.det), 
                    z = apply(hbef2015$y, c(1, 2), max, na.rm = TRUE))
```

In multispecies models, we specify priors on the community-level coefficients rather than the species-level effects.  For nonspatial models, these priors are specified with the following tags: `beta.comm.normal` (normal prior on the community level occurrence mean effects), `alpha.comm.normal` (normal prior on the community level detection mean effects), `tau.beta.ig` (inverse-Gamma prior on the community level occurrence variance parameters), `tau.alpha.ig` (inverse-Gamma prior on the community level detection variance parameters). Each tag consists of a list with elements corresponding to the mean and variance for normal priors and scale and shape for inverse-Gamma priors. 

Below we specify normal priors to be relatively non-informative on the probability scale with a mean of 0 and variance of 2.72, and specify vague inverse gamma priors on the community level variance parameters setting both the shape and scale parameters to 0.1. 

```{r}
ms.priors <- list(beta.comm.normal = list(mean = rep(0, p.occ), 
                                          var = rep(2.72, p.occ)), 
                  alpha.comm.normal = list(mean = rep(0, p.det), 
                                           var = rep(2.72, p.det)), 
                  tau.beta.ig = list(a = rep(0.1, p.occ), 
                                     b = rep(0.1, p.occ)), 
                  tau.alpha.ig = list(a = rep(0.1, p.det), 
                                      b = rep(0.1, p.det)))
```

All that's left to do is specify the number of threads to use (`n.omp.threads`), the number of MCMC samples (`n.samples`), the amount of samples to discard as burn-in (`n.burn`), the the thinning rate (`n.thin`), and arguments to control the display of sampler progress (`verbose`, `n.report`). 

```{r}
out.ms <- msPGOcc(occ.formula = occ.ms.formula, 
		  det.formula = det.ms.formula, 
		  data = hbef2015, 
		  starting = ms.starting, 
		  n.samples = 20000, 
		  priors = ms.priors, 
		  n.omp.threads = 1, 
		  verbose = TRUE, 
		  n.report = 5000, 
		  n.burn = 10000,
		  n.thin = 4)
out.ms$run.time
```

We see `msPGOcc` took less than 3 minutes to run the multispecies occupancy model with 373 sites and 12 species for a total of 20,000 iterations. The resulting object `out.ms` is a list of class `msPGOcc` consisting primarily of posterior samples of all community and species level parameters, as well as some additional objects that are used for summaries, prediction, and model fit evaluation. We can display a nice summary of these results using the `summary` function. For multispecies objects, when using summary we need to specify the level of parameters we want to summarize. We do this using the argument `level`, which takes values `community`, `species`, or `both` to print results for community-level parameters, species-level parameters, or all parameters. 

```{r}
summary(out.ms, level = 'both')
```

Looking at the community level variance parameters, we see large variability in the average occurrence (the intercept) for the twelve species, as well as substantial variability in the effect of elevation across the community. There appears to be less variability across species in the detection portion of the model. We can look directly at the species-specific effects to confirm this.

### Convergence diagnostics

The resulting posterior samples in the `msPGOcc` object are `coda:mcmc` samples, and so convergence diagnostics can proceed as before. 

```{r, fig.height = 4, fig.width = 4, fig.align = 'center'}
plot(out.ms$beta.comm.samples, density = FALSE)
# Look at the first few species-specific occurrence intercepts
plot(out.ms$beta.samples[, 1:4], density = FALSE)
```

Looking at the species-specific intercepts, we would probably want to run the model a bit longer. Formal assessments of convergence using the Gelman-Rubin diagnostic can be accomplished following the steps shown for `PGOcc` using the `gelman.diag` function. 

### Posterior predictive checks

We can use the `ppcOcc` function to perform a posterior predictive check, and summarize the check with a Bayesian p-value using the `summary` function. The `summary` function again requires the `level` argument to specify if you want an overall Bayesian p-value for the entire community (`level = 'community'`), each individual species (`level = 'species'`), or both (`level = 'both'`). 

```{r}
ppc.ms.out <- ppcOcc(out.ms, 'chi-square', group = 1)
summary(ppc.ms.out, level = 'both')
```

The Bayesian p-value for the overall community suggests an adequate model fit, but looking closer at each individual species reveals certain species (the very common species) that our model may not be doing a great job. We should explore this further in a complete analysis (and also of course run the model longer to ensure convergence, as this may be contributing to many of the extreme values). 

### Model selection using WAIC

We can compute the WAIC for comparison with alternative models using the `waicOCC` function. 

```{r}
waicOcc(out.ms)
```

### Prediction

Out-of-sample prediction with `msPGOcc` objects is exactly analogous to what we saw with `PGOcc`. We can use the `predict` function along with a data frame of covariates at new locations. We predict across the entire HBEF for all twelve species using the elevation data stored in `hbefElev`. Instead of predicting for all `r #nrow(hbefElev)` 30 x 30 m cells across the HBEF, below we predict values at every 50th cell. We predict only for OVEN for comparison to predictions using single species occupancy models. 

```{r, fig.width = 5, fig.height = 5, fig.align = 'center'}
X.0.ms <- cbind(1, hbefElev$val, hbefElev$val^2)
X.0.ms <- X.0.ms[seq(1, nrow(X.0.ms), by = 50), ]
out.ms.pred <- predict(out.ms, X.0.ms)
psi.pred.oven <- out.ms.pred$psi.0.samples[, which(sp.names == 'OVEN'), ]
hist(apply(psi.pred.oven, 2, mean), xlab = 'Mean Occurrence Probability')
```
The histogram supports our results from the single species model that indicated OVEN was common throughout most of the HBEF. 

## `spMsPGOcc`: Multispecies spatial occupancy models

Residual spatial autocorrelation may perhaps be more prominent in multispecies occupancy models compared to single species models, as a single set of covariates is used to explain occurrence probability across a region of interest for all species. Given the large variety individual species show in habitat requirements, this may result in important drivers of occurrence probability not being included for certain species, resulting in many species having high residual spatial autocorrelation. The function `spMsPGOcc` fits spatially explicit multispecies occupancy models. Similar to single species models using `spPGOcc`, models can be fit using either a full Gaussian Process (GP) or a Nearest Neighbor Gaussian Process (NNGP). `msSpPGOcc` fits a separate spatial process for each species. The syntax for `msSpPGOcc` is analogous to the syntax for single species spatially-explicit models using `spPGOcc`.

```{r, eval = FALSE}
spMsPGOcc(occ.formula, det.formula, data, starting, n.batch, 
          batch.length, accept.rate = 0.43, priors, 
          cov.model = "exponential", tuning, n.omp.threads = 1, 
          verbose = TRUE, NNGP = TRUE, n.neighbors = 15, 
          search.type = "cb", n.report = 100, 
          n.burn = round(.10 * n.batch * batch.length), n.thin = 1, ...)
```

We will again display the model using the HBEF foliage-gleaning bird data set, with the same predictors in our occurrence and detection models

```{r}
occ.ms.sp.formula <- ~ Elevation + Elevation.2
det.ms.sp.formula <- ~ day + tod + day.2
```

Our starting values in the `starting` argument will look analagous to what we specified for the nonspatial multispecies occupancy model using `msPGOcc`, but we will also include additional starting values for the parameters controlling the spatial processes: `sigma.sq` is the species-specific spatial variance parameter, `phi` is the species specific spatial decay parameter, and `w` is the latent spatial proccess for each species at each site. We will use an exponential covariance model, but when using a Matern covariance model we must also specify starting values for `nu`, the species-specific spatial smoothness parameter. Note that all species-specific spatial parameters are independent of each other. We do not share any information from one spatial process to another. This is something we plan to incorporate for future `spOccupancy` development. Starting values for `phi`, `sigma.sq`, and `nu` (if applicable) are specified as vectors with $N$ elements (the number of species being modeled), while the starting values for the latent spatial processes are specified as a matrix with $N$ rows (i.e., species) and $J$ columns (i.e., sites). Here we set the starting value for the spatial variances equal to 2 for all species and set the starting values for the spatial decay parameter to yield an effective range of the average distance between sites across the HBEF.

```{r}
# Number of species
N <- dim(hbef2015$y)[1]
# Distances between sites
dist.hbef <- dist(hbef2015$coords)
# Number of detection and occupancy regression parameters
p.det <- length(hbef2015$det.covs) + 1
p.occ <- ncol(hbef2015$occ.covs) + 1
# Exponential covariance model
cov.model <- "exponential"
ms.starting <- list(alpha.comm = rep(0, p.det), 
                    beta.comm = rep(0, p.occ), 
                    beta = matrix(0, N, p.occ), 
                    alpha = matrix(0, N, p.det),
                    tau.beta = rep(1, p.occ), 
                    tau.alpha = rep(1, p.det), 
                    z = apply(hbef2015$y, c(1, 2), max, na.rm = TRUE), 
		    sigma.sq = rep(2, N), 
		    phi = rep(3 / mean(dist.hbef), N), 
		    w = matrix(0, N, dim(hbef2015$y)[2]))
```

We next specify the priors in the `priors` argument. The priors are the same as those we specified for the non-spatial multispecies model, with the addition of priors for the parameters controlling the species-specific spatial processes. We assume independent priors for all spatial parameters across the different species. For each species, we assign an inverse gamma prior for the spatial varaince parameter `sigma.sq` (tag is `sigma.sq.ig`) and uniform priors for the spatial decay parameter `phi` and smoothness parameter `nu` (if `cov.model = 'matern'`), with the associated tags `phi.unif` and `nu.unif`. All priors are specified as lists with two elements. For the inverse-Gamma prior, the first element is a length $N$ vector of shape parameters for each species, and the second element is a length $N$ vector of scale parameters for each species. For the uniform priors, the first element is a length $N$ vector of the lower bounds for each species, and the second element is a length $N$ vector of upper bounds for each species. For the inverse-Gamma prior on the spatial variances, here we set the shape parameter to 2 and the scale parameter equal to 2. For a more formal analysis, we would likely want to do some exploratory data analysis to obtain a better guess for the spatial variance for each species, and then replace the scale parameter with this estimated guess for each species. For the spatial decay parameter, we determine the bounds of the uniform distribution by computing the smallest distance between sites and the largest distance between sites. We then set the lower bound of the uniform to `3/max` and the upper bound to `3/min`, where `min` and `max` correspond to the predetermined distances between sites. 

```{r}
# Minimum value is 0, so need to grab second element.
min.dist <- sort(unique(dist.hbef))[2]
max.dist <- max(dist.hbef)
ms.priors <- list(beta.comm.normal = list(mean = rep(0, p.occ), 
                                          var = rep(2.72, p.occ)), 
                  alpha.comm.normal = list(mean = rep(0, p.det), 
                                           var = rep(2.72, p.det)), 
                  tau.beta.ig = list(a = rep(0.1, p.occ), 
                                     b = rep(0.1, p.occ)), 
                  tau.alpha.ig = list(a = rep(0.1, p.det), 
                                      b = rep(0.1, p.det)), 
		  sigma.sq.ig = list(a = rep(2, N), 
				     b = rep(2, N)), 
		  phi.unif = list(a = rep(3/max.dist, N), 
				  b = rep(3/min.dist, N)))
```

We next set the parameters controlling the Adaptive MCMC algorithm (see `spPGOcc` section for details). Notice our specification of the starting tuning values is exactly the same as for `spPGOcc`. We assume the same initial tuning value for all species. However, the adaptive algorithm will allow for species specific tuning parameters, so these will be adjusted in the algorithm as needed. 

```{r}
batch.length <- 25
n.batch <- 400
n.burn <- 2000
n.thin <- 4
ms.tuning <- list(phi = 0.5)
n.omp.threads <- 1
# Values for reporting
verbose <- TRUE
n.report <- 50
```

Spatially explicit multispecies occupancy models are currently the most computationally intensive models fit by `spOccupancy`. Even for modest sized data sets, we encourage the use of NNGPs instead of full GPs when fitting spatially-explicit models to ease the computational burden of fitting these models. We fit the model with an NNGP below and summarize it using the `summary` function, where we specify that we want to summarize both species and community level parameters. 

```{r}
out.sp.ms <- spMsPGOcc(occ.formula = occ.ms.sp.formula, 
		       det.formula = det.ms.sp.formula, 
		       data = hbef2015, 
		       starting = ms.starting, 
		       n.batch = n.batch, 
		       batch.length = batch.length, 
		       accept.rate = 0.43, 
		       priors = ms.priors, 
		       cov.model = cov.model, 
		       tuning = ms.tuning, 
		       n.omp.threads = n.omp.threads, 
		       verbose = TRUE, 
		       NNGP = TRUE, 
		       n.report = n.report, 
		       n.burn = n.burn, 
		       n.thin = n.thin)
summary(out.sp.ms, level = 'both')
```

The resulting object `out.sp.ms` is a list of class `spMsPGOcc` consisting primarily of posterior samples of all community and species-level parameters, as well as some additional objects that are used for summaries, predictions, and model fit evaluation. 

### Convergence diagnostics

Convergence diagnostics proceed as we have seen with all previous `spOccupancy` model objects. Posterior samples are returned as `coda::mcmc` objects, so we can use functions like `plot` and `gelman.diag` to assess convergence.

```{r}
plot(out.sp.ms$beta.comm.samples, density = FALSE)
# Species-specific effects have yet to converge
plot(out.sp.ms$beta.samples[, 1:4], density = FALSE)
```

### Posterior predictive checks

We perform posterior predictive checks to assess Goodness of Fit using `ppcOcc` just as we have previously seen. 

```{r}
ppc.sp.ms.out <- ppcOcc(out.sp.ms, 'freeman-tukey', group = 2)
summary(ppc.sp.ms.out, level = 'both')
```

We see all Bayesian p-values are quite large, which is probably at least partly due to the fact that the chains have yet to converge. 

### Model selection using WAIC

Below we compute the WAIC using `waicOcc` and compare it to the WAIC for the non-spatial multispecies occupancy model. 

```{r}
waicOcc(out.sp.ms)
waicOcc(out.ms)
```

The WAIC for the spatial model is much larger than that for the nonspatial model, potentially indicating we don't need the additional complexities brought in by the species-specific spatial processes. However, we should not place a large emphasis on this since neither of the models has completely converged. 

### Prediction

Out-of-sample prediction with `spMsPGOcc` objects again uses the `predict` function given a set of covariates and spatial coordinates of unobserved locations. Here we predict values for all 12 species at every 50th cell of the total `r #nrow(hbefElev)` cells. We show results only for OVEN for comparison to results with the nonspatial multispecies occupancy model. 

```{r}
X.0.ms <- cbind(1, hbefElev$val, hbefElev$val^2)
X.0.ms <- X.0.ms[seq(1, nrow(X.0.ms), by = 50), ]
coords.0 <- hbefElev[seq(1, nrow(hbefElev), by = 50), 2:3]
out.sp.ms.pred <- predict(out.sp.ms, X.0.ms, coords.0)
psi.pred.oven <- out.sp.ms.pred$psi.0.samples[, which(sp.names == 'OVEN'), ]
hist(apply(psi.pred.oven, 2, mean), xlab = 'Mean Occurrence Probability')
```

## `intPGOcc`: single species integrated occupancy models

### Convergence diagnostics

### Posterior predictive checks

### Model selection using WAIC

### Prediction

## `spIntPGOcc`: single species spatial integrated occupancy models

### Convergence diagnostics

### Posterior predictive checks

### Model selection using WAIC

### Prediction

## References {-}

